{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28 0.42 0.14 0.16]\n",
      "[1]\n",
      "0.983\n"
     ]
    }
   ],
   "source": [
    "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
    "        learning_rate=1.0, n_estimators=100, random_state=0)\n",
    "print(clf.feature_importances_)\n",
    "print(clf.predict([[0, 0, 0, 0]]))\n",
    "print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetune特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_feature=np.load('/home/yangtianyun/MM_course/ft-feature/trainall_feature.npy')\n",
    "test_feature=np.load('/home/yangtianyun/MM_course/ft-feature/test_feature.npy')\n",
    "train_label=np.load('/home/yangtianyun/MM_course/ft-feature/trainall_label.npy')\n",
    "test_label=np.load('/home/yangtianyun/MM_course/ft-feature/test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangtianyun/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                   n_estimators=100, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(train_feature, train_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9805885681946802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    nonrumor     0.9780    0.9828    0.9804      8715\n",
      "       rumor     0.9832    0.9784    0.9808      8955\n",
      "\n",
      "    accuracy                         0.9806     17670\n",
      "   macro avg     0.9806    0.9806    0.9806     17670\n",
      "weighted avg     0.9806    0.9806    0.9806     17670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(train_label, prediction))\n",
    "print(classification_report(train_label, prediction, labels=[0.0, 1.0], target_names=['nonrumor', 'rumor'],digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7580144777662875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    nonrumor     0.7258    0.8825    0.7965       519\n",
      "       rumor     0.8185    0.6138    0.7015       448\n",
      "\n",
      "    accuracy                         0.7580       967\n",
      "   macro avg     0.7721    0.7482    0.7490       967\n",
      "weighted avg     0.7687    0.7580    0.7525       967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction = clf.predict(test_feature)\n",
    "print(accuracy_score(test_label, prediction))\n",
    "print(classification_report(test_label, prediction, labels=[0.0, 1.0], target_names=['nonrumor', 'rumor'],digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging，在训练集上进行样本加权"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_adaboost_clf(Y_train, X_train, Y_test, X_test, M=20, weak_clf=LogisticRegression()):\n",
    "    n_train, n_test = len(X_train), len(X_test)\n",
    "    print(n_train, n_test)\n",
    "    # Initialize weights\n",
    "    w = np.ones(n_train) / n_train\n",
    "    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n",
    "    \n",
    "    for i in range(M):\n",
    "        # Fit a classifier with the specific weights\n",
    "        weak_clf.fit(X_train, Y_train, sample_weight = w)\n",
    "        pred_train_i = weak_clf.predict(X_train)\n",
    "        pred_test_i = weak_clf.predict(X_test)\n",
    "        print(pred_train_i.shape,pred_test_i.shape)\n",
    "        print(Y_train.shape,Y_test.shape)\n",
    "        \n",
    "        # Indicator function\n",
    "        miss = [int(x) for x in (pred_train_i != Y_train)]\n",
    "        print(\"weak_clf_%02d train acc: %.4f\" % (i + 1, 1 - sum(miss) / n_train))\n",
    "        \n",
    "        # Error\n",
    "        err_m = np.dot(w, miss)\n",
    "        # Alpha\n",
    "        alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))\n",
    "        # New weights\n",
    "        miss2 = [x if x==1 else -1 for x in miss] # -1 * y_i * G(x_i): 1 / -1\n",
    "        w = np.multiply(w, np.exp([float(x) * alpha_m for x in miss2]))\n",
    "        w = w / sum(w)\n",
    "\n",
    "        # Add to prediction\n",
    "        pred_train_i = [1 if x == 1 else -1 for x in pred_train_i]\n",
    "        pred_test_i = [1 if x == 1 else -1 for x in pred_test_i]\n",
    "        pred_train = pred_train + np.multiply(alpha_m, pred_train_i)\n",
    "        pred_test = pred_test + np.multiply(alpha_m, pred_test_i)\n",
    "    \n",
    "    pred_train = (pred_train > 0) * 1\n",
    "    pred_test = (pred_test > 0) * 1\n",
    "\n",
    "    print(\"My AdaBoost clf train accuracy: %.4f\" % (sum(pred_train == Y_train) / n_train))\n",
    "    print(\"My AdaBoost clf test accuracy: %.4f\" % (sum(pred_test == Y_test) / n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_feature=np.load('/home/yangtianyun/MM_course/ft-feature/trainall_feature.npy')\n",
    "test_feature=np.load('/home/yangtianyun/MM_course/ft-feature/test_feature.npy')\n",
    "train_label=np.load('/home/yangtianyun/MM_course/ft-feature/trainall_label.npy')[:,0]\n",
    "test_label=np.load('/home/yangtianyun/MM_course/ft-feature/test_label.npy')[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17670 967\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_01 train acc: 0.9768\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_02 train acc: 0.9168\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_03 train acc: 0.9098\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_04 train acc: 0.7916\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_05 train acc: 0.7246\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_06 train acc: 0.5496\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_07 train acc: 0.6549\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_08 train acc: 0.5390\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_09 train acc: 0.6686\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_10 train acc: 0.6833\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_11 train acc: 0.8233\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_12 train acc: 0.4161\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_13 train acc: 0.6009\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_14 train acc: 0.6327\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_15 train acc: 0.8043\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_16 train acc: 0.4495\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_17 train acc: 0.6013\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_18 train acc: 0.6634\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_19 train acc: 0.4273\n",
      "(17670,) (967,)\n",
      "(17670,) (967,)\n",
      "weak_clf_20 train acc: 0.7857\n",
      "My AdaBoost clf train accuracy: 0.9791\n",
      "My AdaBoost clf test accuracy: 0.7694\n"
     ]
    }
   ],
   "source": [
    "my_adaboost_clf(train_label,train_feature,test_label,test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练集随机采样，然后将在测试集上高于baseline的分类器bagging\n",
    "\n",
    "在funetune好的feature上分类，只需要很少的样本就能分出分界面，而且分界面应该是差不多的\n",
    "\n",
    "选择出来的在测试集准确率高的样本是什么样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_feature=np.load('/home/yangtianyun/MM_course/ft-feature/trainall_feature.npy')\n",
    "test_feature=np.load('/home/yangtianyun/MM_course/ft-feature/test_feature.npy')\n",
    "train_label=np.load('/home/yangtianyun/MM_course/ft-feature/trainall_label.npy')[:,0]\n",
    "test_label=np.load('/home/yangtianyun/MM_course/ft-feature/test_label.npy')[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tsne(vis_x,vis_y,embeddings,Y_test,pred_test_i):\n",
    "  \n",
    "    A= Y_test==pred_test_i\n",
    "    B= Y_test==1\n",
    "    C= Y_test==0\n",
    "    embeddings_rumor=embeddings[A & B]\n",
    "    embeddings_nonrumor=embeddings[A & C]\n",
    "    x = embeddings_rumor[:, 0]\n",
    "    y = embeddings_rumor[:, 1]\n",
    "    x2 = embeddings_nonrumor[:, 0]\n",
    "    y2 = embeddings_nonrumor[:, 1]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(vis_x, vis_y, c=Y_test, cmap=plt.cm.get_cmap(\"jet\", 2), marker='.')\n",
    "    plt.scatter(x,y, marker='o',c='',edgecolors='g',s=100)\n",
    "    plt.scatter(x2,y2, marker='o',c='',edgecolors='y',s=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def my_adaboost_clf(Y_train, X_train, Y_test, X_test, M=100):\n",
    "    n_train, n_test = len(X_train), len(X_test)\n",
    "    print(n_train, n_test)\n",
    "    pred_train, pred_test = [np.zeros(n_train), np.zeros(n_test)]\n",
    "\n",
    "    rate=0.001\n",
    "    w = np.ones(n_train) / n_train\n",
    "    \n",
    "    train_rumor_feature=X_train[Y_train==1]\n",
    "    train_nonrumor_feature=X_train[Y_train==0]\n",
    "    n_train_rumor=len(train_rumor_feature)\n",
    "    n_train_nonrumor=len(train_nonrumor_feature)\n",
    "    \n",
    "    embeddings = TSNE(n_jobs=4).fit_transform(X_test)\n",
    "    vis_x = embeddings[:, 0]\n",
    "    vis_y = embeddings[:, 1]\n",
    "    acc_max=0\n",
    "    for i in range(M):\n",
    "        print(i)\n",
    "        # Fit a classifier with the specific weights\n",
    "        weak_clf=LogisticRegression()\n",
    "        train_rumor_index=random.sample(range(n_train_rumor),int(n_train_rumor*rate))\n",
    "        train_nonrumor_index=random.sample(range(n_train_nonrumor),int(n_train_nonrumor*rate))\n",
    "        train_rumor_sample=train_rumor_feature[train_rumor_index]\n",
    "        train_nonrumor_sample=train_nonrumor_feature[train_nonrumor_index]\n",
    "        \n",
    "        X_train_sample=np.concatenate([train_rumor_sample,train_nonrumor_sample])\n",
    "        Y_train_sample=np.concatenate([np.ones([int(n_train_rumor*rate)]),np.zeros([int(n_train_nonrumor*rate)])])\n",
    "        \n",
    "        weak_clf.fit(X_train_sample, Y_train_sample)\n",
    "        pred_train_i_ori = weak_clf.predict(X_train)\n",
    "        pred_test_i_ori = weak_clf.predict(X_test)\n",
    "\n",
    "        # Indicator function\n",
    "        miss_train = [int(x) for x in (pred_train_i_ori != Y_train)]\n",
    "        #print(\"weak_clf_%02d train acc: %.4f\" % (i + 1, 1 - sum(miss_train) / n_train))\n",
    "\n",
    "        miss_test = [int(x) for x in (pred_test_i_ori != Y_test)]\n",
    "#         print(\"weak_clf_%02d test acc: %.4f\" % (i + 1, 1 - sum(miss_test) / n_test))\n",
    "        \n",
    "        # Error\n",
    "#         err_m_t = np.dot(w, miss_train)\n",
    "        err_m=sum(miss_test) / n_test\n",
    "        \n",
    "\n",
    "        if(err_m<0.24):\n",
    "            \n",
    "            \n",
    "            # Alpha\n",
    "            alpha_m = 0.5 * np.log((1 - err_m) / float(err_m))\n",
    "\n",
    "            # Add to prediction\n",
    "            pred_train_i = [1 if x == 1 else -1 for x in pred_train_i_ori]\n",
    "            pred_test_i = [1 if x == 1 else -1 for x in pred_test_i_ori]\n",
    "\n",
    "            pred_train = pred_train + np.multiply(alpha_m, pred_train_i)\n",
    "            pred_test = pred_test + np.multiply(alpha_m, pred_test_i)\n",
    "\n",
    "            pred_train_f = (pred_train > 0) * 1\n",
    "            pred_test_f = (pred_test > 0) * 1\n",
    "\n",
    "            acc=sum(pred_test_f == Y_test) / n_test\n",
    "\n",
    "            if acc> acc_max:\n",
    "                acc_max=acc\n",
    "#                 show_tsne(vis_x,vis_y,embeddings,Y_test,pred_test_i_ori)\n",
    "\n",
    "            print(\"My AdaBoost clf test accuracy: %.4f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17670 967\n",
      "0\n",
      "1\n",
      "My AdaBoost clf test accuracy: 0.7622\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "My AdaBoost clf test accuracy: 0.7601\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "My AdaBoost clf test accuracy: 0.7663\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "My AdaBoost clf test accuracy: 0.7777\n",
      "28\n",
      "29\n",
      "30\n",
      "My AdaBoost clf test accuracy: 0.7756\n",
      "31\n",
      "32\n",
      "33\n",
      "My AdaBoost clf test accuracy: 0.7818\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "My AdaBoost clf test accuracy: 0.7766\n",
      "38\n",
      "My AdaBoost clf test accuracy: 0.7818\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "My AdaBoost clf test accuracy: 0.7797\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "My AdaBoost clf test accuracy: 0.7797\n",
      "53\n",
      "54\n",
      "55\n",
      "My AdaBoost clf test accuracy: 0.7818\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "My AdaBoost clf test accuracy: 0.7818\n",
      "71\n",
      "72\n",
      "My AdaBoost clf test accuracy: 0.7787\n",
      "73\n",
      "74\n",
      "My AdaBoost clf test accuracy: 0.7859\n",
      "75\n",
      "My AdaBoost clf test accuracy: 0.7828\n",
      "76\n",
      "77\n",
      "78\n",
      "My AdaBoost clf test accuracy: 0.7921\n",
      "79\n",
      "80\n",
      "81\n",
      "My AdaBoost clf test accuracy: 0.7880\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "My AdaBoost clf test accuracy: 0.7901\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "My AdaBoost clf test accuracy: 0.7880\n",
      "95\n",
      "96\n",
      "97\n",
      "My AdaBoost clf test accuracy: 0.7901\n",
      "98\n",
      "99\n",
      "My AdaBoost clf test accuracy: 0.7890\n"
     ]
    }
   ],
   "source": [
    "my_adaboost_clf(train_label,train_feature,test_label,test_feature)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
