{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32193, 55), (1613, 55), (32193, 2), (1613, 2))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_se = np.load('./data/train_emotion_(32193, 55).npy')\n",
    "test_se = np.load('./data/test_emotion_(1613, 55).npy')\n",
    "train_label = np.load('../dataset/data/train_label_(32193, 2).npy')\n",
    "test_label = np.load('../dataset/data/test_label_(1613, 2).npy')\n",
    "\n",
    "train_se.shape, test_se.shape, train_label.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = train_se, test_se\n",
    "y_train, y_test = train_label, test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(seed=0): \n",
    "    clf = RandomForestClassifier(random_state=seed)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    test_label = y_test\n",
    "    \n",
    "    auc = roc_auc_score(test_label, y_pred)\n",
    "\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "\n",
    "    accuracy = accuracy_score(test_label, y_pred)\n",
    "    eval_dict = classification_report(test_label, y_pred, labels=[0, 1],\n",
    "                                      target_names=['truth', 'rumor'], output_dict=True)\n",
    "\n",
    "    print()\n",
    "    print('TEST_sz:', len(test_label))\n",
    "#     print('test: {}+, {}-'.format(int(sum(test_label)), int(len(test_label) - sum(test_label))))\n",
    "    print()\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    print('AUC: {}'.format(auc))\n",
    "#     print('Confusion Matrix:\\n {}'.format(confusion_matrix(test_label, y_pred)))\n",
    "    print()\n",
    "    print(classification_report(test_label, y_pred, labels=[0, 1],\n",
    "                                target_names=['truth', 'rumor'], digits=3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6466212027278363\n",
      "AUC: 0.6170006466636199\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.622     0.958     0.755       891\n",
      "       rumor      0.859     0.262     0.401       722\n",
      "\n",
      "   micro avg      0.655     0.647     0.651      1613\n",
      "   macro avg      0.741     0.610     0.578      1613\n",
      "weighted avg      0.728     0.647     0.597      1613\n",
      " samples avg      0.647     0.647     0.647      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6497210167389956\n",
      "AUC: 0.6185648420182123\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.624     0.956     0.755       891\n",
      "       rumor      0.841     0.271     0.410       722\n",
      "\n",
      "   micro avg      0.655     0.650     0.653      1613\n",
      "   macro avg      0.732     0.614     0.583      1613\n",
      "weighted avg      0.721     0.650     0.601      1613\n",
      " samples avg      0.650     0.650     0.650      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6602603843769373\n",
      "AUC: 0.6302242181743567\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.631     0.960     0.761       891\n",
      "       rumor      0.864     0.291     0.435       722\n",
      "\n",
      "   micro avg      0.666     0.660     0.663      1613\n",
      "   macro avg      0.748     0.625     0.598      1613\n",
      "weighted avg      0.735     0.660     0.615      1613\n",
      " samples avg      0.660     0.660     0.660      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6497210167389956\n",
      "AUC: 0.6173111540147551\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.622     0.955     0.753       891\n",
      "       rumor      0.838     0.273     0.412       722\n",
      "\n",
      "   micro avg      0.654     0.650     0.652      1613\n",
      "   macro avg      0.730     0.614     0.583      1613\n",
      "weighted avg      0.719     0.650     0.600      1613\n",
      " samples avg      0.650     0.650     0.650      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6565406075635462\n",
      "AUC: 0.6291974842298019\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.631     0.948     0.758       891\n",
      "       rumor      0.846     0.296     0.439       722\n",
      "\n",
      "   micro avg      0.665     0.657     0.661      1613\n",
      "   macro avg      0.738     0.622     0.598      1613\n",
      "weighted avg      0.727     0.657     0.615      1613\n",
      " samples avg      0.657     0.657     0.657      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6484810911345319\n",
      "AUC: 0.6197349767294364\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.625     0.953     0.755       891\n",
      "       rumor      0.845     0.273     0.413       722\n",
      "\n",
      "   micro avg      0.657     0.648     0.653      1613\n",
      "   macro avg      0.735     0.613     0.584      1613\n",
      "weighted avg      0.724     0.648     0.602      1613\n",
      " samples avg      0.648     0.648     0.648      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6503409795412275\n",
      "AUC: 0.6200812371172482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.625     0.954     0.755       891\n",
      "       rumor      0.843     0.276     0.415       722\n",
      "\n",
      "   micro avg      0.657     0.650     0.654      1613\n",
      "   macro avg      0.734     0.615     0.585      1613\n",
      "weighted avg      0.722     0.650     0.603      1613\n",
      " samples avg      0.650     0.650     0.650      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6472411655300682\n",
      "AUC: 0.6174246310442063\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.624     0.952     0.754       891\n",
      "       rumor      0.824     0.271     0.408       722\n",
      "\n",
      "   micro avg      0.654     0.647     0.651      1613\n",
      "   macro avg      0.724     0.612     0.581      1613\n",
      "weighted avg      0.714     0.647     0.599      1613\n",
      " samples avg      0.647     0.647     0.647      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6453812771233726\n",
      "AUC: 0.6147381012339461\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.622     0.953     0.752       891\n",
      "       rumor      0.831     0.266     0.403       722\n",
      "\n",
      "   micro avg      0.652     0.645     0.649      1613\n",
      "   macro avg      0.726     0.609     0.578      1613\n",
      "weighted avg      0.715     0.645     0.596      1613\n",
      " samples avg      0.645     0.645     0.645      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6484810911345319\n",
      "AUC: 0.620839434666766\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.626     0.952     0.755       891\n",
      "       rumor      0.846     0.274     0.414       722\n",
      "\n",
      "   micro avg      0.658     0.648     0.653      1613\n",
      "   macro avg      0.736     0.613     0.585      1613\n",
      "weighted avg      0.724     0.648     0.603      1613\n",
      " samples avg      0.648     0.648     0.648      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snow/anaconda2/envs/python35/lib/python3.5/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for seed in range(10):\n",
    "    train(seed)\n",
    "    print('\\n===========================================================\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(seed=0):\n",
    "    dt = DecisionTreeClassifier(random_state=seed)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = dt.predict(X_test)\n",
    "    test_label = y_test\n",
    "    \n",
    "    auc = roc_auc_score(test_label, y_pred)\n",
    "\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "\n",
    "    accuracy = accuracy_score(test_label, y_pred)\n",
    "    eval_dict = classification_report(test_label, y_pred, labels=[0, 1],\n",
    "                                      target_names=['truth', 'rumor'], output_dict=True)\n",
    "\n",
    "    print()\n",
    "    print('TEST_sz:', len(test_label))\n",
    "#     print('test: {}+, {}-'.format(int(sum(test_label)), int(len(test_label) - sum(test_label))))\n",
    "    print()\n",
    "    print('Accuracy: {}'.format(accuracy))\n",
    "    print('AUC: {}'.format(auc))\n",
    "#     print('Confusion Matrix:\\n {}'.format(confusion_matrix(test_label, y_pred)))\n",
    "    print()\n",
    "    print(classification_report(test_label, y_pred, labels=[0, 1],\n",
    "                                target_names=['truth', 'rumor'], digits=3))\n",
    "    print()\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6373217606943583\n",
      "AUC: 0.6097182971605871\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.622     0.873     0.727       891\n",
      "       rumor      0.689     0.346     0.461       722\n",
      "\n",
      "   micro avg      0.637     0.637     0.637      1613\n",
      "   macro avg      0.656     0.610     0.594      1613\n",
      "weighted avg      0.652     0.637     0.608      1613\n",
      " samples avg      0.637     0.637     0.637      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6391816491010539\n",
      "AUC: 0.6133721020609293\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.626     0.860     0.725       891\n",
      "       rumor      0.679     0.367     0.477       722\n",
      "\n",
      "   micro avg      0.639     0.639     0.639      1613\n",
      "   macro avg      0.653     0.613     0.601      1613\n",
      "weighted avg      0.650     0.639     0.614      1613\n",
      " samples avg      0.639     0.639     0.639      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.624922504649721\n",
      "AUC: 0.5983635990561198\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.616     0.852     0.715       891\n",
      "       rumor      0.654     0.345     0.451       722\n",
      "\n",
      "   micro avg      0.625     0.625     0.625      1613\n",
      "   macro avg      0.635     0.598     0.583      1613\n",
      "weighted avg      0.633     0.625     0.597      1613\n",
      " samples avg      0.625     0.625     0.625      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6280223186608803\n",
      "AUC: 0.6010380816474999\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.617     0.859     0.718       891\n",
      "       rumor      0.663     0.343     0.453       722\n",
      "\n",
      "   micro avg      0.628     0.628     0.628      1613\n",
      "   macro avg      0.640     0.601     0.585      1613\n",
      "weighted avg      0.638     0.628     0.599      1613\n",
      " samples avg      0.628     0.628     0.628      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.634221946683199\n",
      "AUC: 0.6079632894037326\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.622     0.859     0.722       891\n",
      "       rumor      0.672     0.357     0.467       722\n",
      "\n",
      "   micro avg      0.634     0.634     0.634      1613\n",
      "   macro avg      0.647     0.608     0.594      1613\n",
      "weighted avg      0.645     0.634     0.607      1613\n",
      " samples avg      0.634     0.634     0.634      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6373217606943583\n",
      "AUC: 0.6110318326384809\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.624     0.862     0.724       891\n",
      "       rumor      0.679     0.360     0.471       722\n",
      "\n",
      "   micro avg      0.637     0.637     0.637      1613\n",
      "   macro avg      0.652     0.611     0.597      1613\n",
      "weighted avg      0.649     0.637     0.611      1613\n",
      " samples avg      0.637     0.637     0.637      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6379417234965902\n",
      "AUC: 0.6114616463185254\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.624     0.864     0.725       891\n",
      "       rumor      0.682     0.359     0.470       722\n",
      "\n",
      "   micro avg      0.638     0.638     0.638      1613\n",
      "   macro avg      0.653     0.611     0.598      1613\n",
      "weighted avg      0.650     0.638     0.611      1613\n",
      " samples avg      0.638     0.638     0.638      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6391816491010539\n",
      "AUC: 0.6124526272264037\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.625     0.868     0.727       891\n",
      "       rumor      0.686     0.357     0.470       722\n",
      "\n",
      "   micro avg      0.639     0.639     0.639      1613\n",
      "   macro avg      0.656     0.612     0.598      1613\n",
      "weighted avg      0.652     0.639     0.612      1613\n",
      " samples avg      0.639     0.639     0.639      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6391816491010539\n",
      "AUC: 0.6135034556087188\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.627     0.859     0.724       891\n",
      "       rumor      0.679     0.368     0.478       722\n",
      "\n",
      "   micro avg      0.639     0.639     0.639      1613\n",
      "   macro avg      0.653     0.614     0.601      1613\n",
      "weighted avg      0.650     0.639     0.614      1613\n",
      " samples avg      0.639     0.639     0.639      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n",
      "\n",
      "TEST_sz: 1613\n",
      "\n",
      "Accuracy: 0.6404215747055176\n",
      "AUC: 0.6138376687776503\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       truth      0.626     0.868     0.727       891\n",
      "       rumor      0.688     0.360     0.473       722\n",
      "\n",
      "   micro avg      0.640     0.640     0.640      1613\n",
      "   macro avg      0.657     0.614     0.600      1613\n",
      "weighted avg      0.654     0.640     0.613      1613\n",
      " samples avg      0.640     0.640     0.640      1613\n",
      "\n",
      "\n",
      "\n",
      "===========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed in range(10):\n",
    "    train(seed)\n",
    "    print('\\n===========================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
